---
layout: post
title:  "Week 9: Information Theory and Early Computers"
date:   2017-10-22 09:27:24 -0400
categories: ['New Media', 'Information']
---
Claude Shannon revolutionized the way we think about the concept of information by defining it as uncertainty (or entropy/randomness) and making that definition quantifiable. Gleick explains how Shannon produced the formula for representing information: 

>“The point was to represent a message as the outcome of a process that generated events with discrete probabilities. Then what could be said about the amount of information, or the rate at which information is generated? For each event, the possible choices each have a known probability (represented as p1, p2, p3, and so on). Shannon wanted to define the measure of information (represented as H) as the measure of uncertainly: ‘of how much ‘choice’  is involved in the selection of the event or of how uncertain we are of the outcome.’ The probabilities might be the same or different, but generally more choices mean more uncertainty—more information. Choices might be broken down into successive choices, with their own probabilities, and the probabilities had to be additive; for example, the probability of a particular digram should be the a weighted sum of the probabilities of the individual symbols.” (228)  

![info](https://dennisdjones.files.wordpress.com/2013/04/shannon-info2.gif)

**What do you think is at stake in conceptualizing information mathematically? What does it do to how we think about the relationship between information and media?** 


